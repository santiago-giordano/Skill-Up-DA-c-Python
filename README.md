# Proyecto #1 Flujos de ejecuci√≥n
## Descripci√≥n
Client: Ministerio de Educaci√≥n de la Naci√≥n
Situaci√≥n inicial
üìç
Somos un equipo de desarrollo y data analytics, que trabajamos para la consultora ‚ÄúMyData‚Äù
y nuestro l√≠der t√©cnico nos comparte un pedido comercial directamente del Consejo Nacional
de Calidad de la Educaci√≥n (por sus siglas, CNCE).

El CNCE es un grupo deliberante que pertenece al Ministerio de Educaci√≥n de la Naci√≥n
Argentina. 

Este se encuentra analizando opciones universitarias disponibles en los √∫ltimos 10
a√±os para comparar datos extra√≠dos de universidades de todo el pa√≠s, p√∫blicas y privadas,
con el fin de tener una muestra representativa que facilite el an√°lisis.
Para esto, compartieron a ‚ÄúMyData‚Äù informaci√≥n disponible de m√°s de 15 universidades y
centros educativos con gran volumen de datos sensibles sobre las inscripciones de alumnos.
El CNCE requiere que preparemos el set de datos para que puedan analizar la informaci√≥n
relevante y tomar directrices en cuanto a qu√© carreras universitarias requieren programa de
becas, qu√© planes de estudios tienen adhesi√≥n, entre otros.

### Tu objetivo

üìã Como parte de un equipo de desarrollo y data analytics de ‚ÄúMyData‚Äù, deber√°s analizar y
preparar flujos de ejecuci√≥n del set de datos recibido para obtener las comparaciones y
mediciones requeridas por el CNCE.

#### Requerimientos üîß

‚óè El Ministerio necesita que ordenemos los datos para obtener un archivo con s√≥lo la
informaci√≥n necesaria de cierto periodo de tiempo y de determinados lugares
geogr√°ficos de una base de datos SQL (las especificaciones ser√°n vistas en la primera
reuni√≥n de equipo). Ser√° necesario generar un diagrama de base de datos para que se
comprenda la estructura.


‚óè Los datos deben ser procesados de manera que se puedan ejecutar consultas a dos
universidades del total disponible para hacer an√°lisis parciales. Para esto ser√°
necesario realizar DAGs con Airflow que permitan procesar datos con Python y
consultas SQL.


‚óè Calcular, evaluar y ajustar formatos de determinados datos como fechas, nombres,
c√≥digos postales seg√∫n requerimientos normalizados que se especifican para cada
grupo de universidades, utilizando Pandas.

### Assets üé®


La base de datos con la informaci√≥n que reuni√≥ el Ministerio de Educaci√≥n se encuentra aqu√≠:

‚óè A proveer en el transcurso del proyecto.


El archivo auxiliar de c√≥digos postales se encuentra en la carpeta assets.


## Requerimientos:
- Apache-Airflow 2.2.2
- Python 3.6
## Modulos utilizados en Python
- pathlib
- logging
- pandas
- datetime
- os
- sqlalchemy

## Enlaces:
- Guia de instalaci√≥n de Apache Airflow en Ubuntu: https://unixcop.com/how-to-install-apache-airflow-on-ubuntu-20

## Estructura y flujo de ejecuci√≥n
  Se generar√°n archivos ".sql" con las consultas correspondientes a cada centro educativo, normalizando las columnas tenidas en cuenta.

  Mediante operadores disponibles en apache airflow (Python operators y postgre operators, se toman las consultas ".sql" para obtener los datos de la base de datos provista. 
  
  Estos datos se transorman mediante la libreria pandas, y se almacenan en forma local como archivos ".txt".

  Finalmete, a traves de las herramientas provistas por AWS (operadores y hooks S3), los datos almacenados como ".txt" son transformados a strings, y almacenados en el servicio S3.

# **Convenci√≥n para nombrar carpetas**

OT000-python

   -airflow

      -assets: archivos complementarios necesarios.

      -dags: para dejar los flujos que se vayan creando

      -datasets: para dejar el archivo resultante del proceso de transformaci√≥n con Pandas
      
      -files: para almacenar la extracci√≥n de la base de datos.

      -include: para almacenar los SQL.


# **Convenci√≥n para nombrar archivos**
### DAG ETL
Se colocar√° grupo-letra-siglas de la universidad y localidad, seguido por "_dag_elt.py" para diferenciar de otros csv.

EJ: GFUNRioCuarto_dag_etl.py


# **Convencion para el nombre de la base de datos**

### conexion con base de datos
se llamara 'alkemy_db'

### conexion para S3
se llamara 'aws_s3_bucket'

### csv generados
Se colocar√° grupo-letra-siglas de la universidad y localidad, seguido por "_select.csv" para diferenciar el dag realizado.

EJ: GFUNRioCuarto_select.csv

### txt generados
Se colocar√° grupo-letra-siglas de la universidad y localidad, seguido por "_process.txt" para diferenciar el dag realizado.

EJ: GFUNRioCuarto_process.txt